{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13QJ2U-Nc3zQ"
   },
   "source": [
    "# Image Captioning - ResNet-LSTM model\n",
    "\n",
    "*Last Updated: 01/31/2022*\n",
    "\n",
    "This note book use a ResNet-LSTM model for image captioning\n",
    "\n",
    "\n",
    "**References**\n",
    " \n",
    "1. Udacity's Computer Vision Nanodegree\n",
    "2. https://github.com/cocodataset/cocoapi\n",
    "3. https://colab.research.google.com/github/rammyram/image_captioning/blob/master/Image_Captioning.ipynb\n",
    "4. https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "Please connect to your Google drive before running this notebook.\n",
    "\n",
    "Please download the COCO dataset with the following notebook:\n",
    "\n",
    "https://github.com/sliao-mi-luku/Image-Captioning/blob/main/ImageCaptioning_download_COCO2014.ipynb\n",
    "\n",
    "**Note**\n",
    "\n",
    "This notebook is run on my local machine (Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTqjEOdPdmRW"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7pqsXb5bTS1",
    "outputId": "6523c2fd-ac6e-4143-afbf-ddcd120da3e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PandaEgg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler\n",
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/opt/cocoapi/PythonAPI\")\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs1_DKZ-DHzk"
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "We built the vocabulary from the training data set (`captions_train2014.json`).\n",
    "\n",
    "Implementation of the vocabulary object is adapted from *Udacity Computer Vision Nanodegree*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sTXk5s_9DLcb"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Vocabulary object (Implemented by Udacity)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_threshold, vocab_file='/content/vocab.pkl',\n",
    "                 start_word=\"<start>\", end_word=\"<end>\", unk_word=\"<unk>\",\n",
    "                 annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                 vocab_from_file=False):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            vocab_threshold: minimum count of the words to be considered a unique token\n",
    "            vocab_file: vocab file\n",
    "            start_word: start-of-sentence token\n",
    "            end_word: end-of-sentence token\n",
    "            unk_word: unknown-word token\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            vocab_from_file: (boolean) whether or not to use the existing vocab file\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"\n",
    "        Load/Create the vocab file\n",
    "        \"\"\"\n",
    "        # load and use the existing vocab file\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        \n",
    "        # build a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"\n",
    "        Create dicts for converting tokens to integers (and vice-versa)\n",
    "        \"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"\n",
    "        Initialize the dictionaries for converting tokens to integers (and vice-versa)\n",
    "        \"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a token to the vocabulary\n",
    "        \"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"\n",
    "        Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold\n",
    "        \"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iePu7YVxJA_8"
   },
   "source": [
    "### Build Vocab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE5sWnnVJBOd",
    "outputId": "ffb73ac0-2668-4912-8007-42d4c64ac02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.02s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "# minimum count required to add to the vocabulary list\n",
    "VOCAB_THRESHOLD = 5\n",
    "\n",
    "# build vocab file from training data\n",
    "train_vocab = Vocabulary(vocab_threshold=VOCAB_THRESHOLD,\n",
    "                         vocab_file=\"./vocab.pkl\",\n",
    "                         start_word=\"<start>\",\n",
    "                         end_word=\"<end>\",\n",
    "                         unk_word=\"<unk>\",\n",
    "                         annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnvTC_5QNYYD"
   },
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d6KFoO1fNb9Y"
   },
   "outputs": [],
   "source": [
    "# training data transform\n",
    "transform_train = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "# validation/test data transform\n",
    "transform_eval = transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsXi7zNJCkNd"
   },
   "source": [
    "## Custom COCO Datasets\n",
    "\n",
    "There are 2 types of cumstom COCO dataset.\n",
    "\n",
    "**Dev Mode**\n",
    "\n",
    "When we loop over the `CoCoDataset_DevMode` dataset, the **processed image tensor** and the **tokenized captions** will be extracted\n",
    "\n",
    "**Caption Mode**\n",
    "\n",
    "When we loop over the `CoCoDataset_CaptionMode` dataset, the **original image matrix** (ndarray) and the **processed image tensor** will be extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-fE0SOCBFy6"
   },
   "source": [
    "### COCO Dataset (Dev Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pbPKNV6xITy4"
   },
   "outputs": [],
   "source": [
    "class CoCoDataset_DevMode(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, batch_size, vocab_file, annotations_file, img_folder):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            transform: data transform\n",
    "            batch_size: batch size\n",
    "            vocab_file: path to the existing vocab file\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            img_folder: path to the images\n",
    "        \"\"\"\n",
    "        # data transform\n",
    "        self.transform = transform\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # vocab\n",
    "        self.vocab = vocab_file\n",
    "        # image folder\n",
    "        self.img_folder = img_folder\n",
    "        # initialize COCO\n",
    "        self.coco = COCO(annotations_file)\n",
    "        # annotation ids\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        print('Obtaining caption lengths...')\n",
    "        all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "        self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th item from the dataset\n",
    "        Returns\n",
    "            image: processed image\n",
    "            caption: tokenized caption (including special tokens)\n",
    "        \"\"\"\n",
    "        ann_id = self.ids[idx]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        # Convert caption to tensor of word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab(self.vocab.start_word))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab(self.vocab.end_word))\n",
    "        caption = torch.Tensor(caption).long()\n",
    "        # return pre-processed image and caption tensors\n",
    "        return image, caption\n",
    "\n",
    "    def get_data_indices(self):\n",
    "        # choose a length of the caption\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        # find all availalbe captions with this length\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        # select batch_size captions among them\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlyeVXa2BMt6"
   },
   "source": [
    "### Create Dev Mode Dataset for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fIikCa0BQBj",
    "outputId": "f4ff8c59-5f90-47da-fa70-7a54da21b8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:43<00:00, 9490.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 202654/202654 [00:20<00:00, 9820.51it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# training dataset for training purposes\n",
    "dataset_train = CoCoDataset_DevMode(transform=transform_train,  # with augmentation\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      vocab_file=train_vocab,\n",
    "                                      annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                                      img_folder=\"./images/train2014/\")\n",
    "\n",
    "training_batch_sampler = BatchSampler(sampler=SubsetRandomSampler(indices=dataset_train.get_data_indices()),\n",
    "                                      batch_size=dataset_train.batch_size,\n",
    "                                      drop_last=False)\n",
    "\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_sampler=training_batch_sampler)\n",
    "\n",
    "\n",
    "# validation dataset for training purposes\n",
    "dataset_val = CoCoDataset_DevMode(transform=transform_eval,  # no augmentation\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  vocab_file=train_vocab,\n",
    "                                  annotations_file=\"./opt/cocoapi/annotations/captions_val2014.json\",\n",
    "                                  img_folder=\"./images/val2014/\")\n",
    "\n",
    "val_batch_sampler = BatchSampler(sampler=SubsetRandomSampler(indices=dataset_val.get_data_indices()),\n",
    "                                 batch_size=dataset_val.batch_size,\n",
    "                                 drop_last=False)\n",
    "\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_sampler=val_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN8C4TwVC_0w",
    "outputId": "1b062af0-ad95-416e-dd9c-a50b16a7d345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 8852\n"
     ]
    }
   ],
   "source": [
    "# size of vocab\n",
    "VOCAB_SIZE = len(dataloader_train.dataset.vocab)\n",
    "print(\"VOCAB_SIZE: {}\".format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset for BLEU calculation\n",
    "single_dataset_train = CoCoDataset_DevMode(transform=transform_eval,  # no augmentation\n",
    "                                           batch_size=1,\n",
    "                                           vocab_file=train_vocab,\n",
    "                                           annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                                           img_folder=\"./images/train2014/\")\n",
    "\n",
    "single_dataloader_train = DataLoader(dataset=single_dataset_train, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# for BLEU calculation\n",
    "single_dataset_val = CoCoDataset_DevMode(transform=transform_eval,  # no augmentation\n",
    "                                         batch_size=1,\n",
    "                                         vocab_file=train_vocab,\n",
    "                                         annotations_file=\".opt/cocoapi/annotations/captions_val2014.json\",\n",
    "                                         img_folder=\"./images/val2014/\")\n",
    "\n",
    "single_dataloader_val = DataLoader(dataset=single_dataset_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n05g5AVACJUL"
   },
   "source": [
    "### COCO Dataset (Caption Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQxJfjtCJ_gu"
   },
   "outputs": [],
   "source": [
    "class CoCoDataset_CaptionMode(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, batch_size, vocab_file, annotations_file, img_folder):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            transform: data transform\n",
    "            batch_size: batch size\n",
    "            vocab_file: path to the existing vocab file\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            img_folder: path to the images\n",
    "        \"\"\"\n",
    "        # data transform\n",
    "        self.transform = transform\n",
    "         # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # vocab\n",
    "        self.vocab = vocab_file\n",
    "        # image folder\n",
    "        self.img_folder = img_folder\n",
    "        # path\n",
    "        test_info = json.loads(open(annotations_file).read())\n",
    "        self.paths = [item['file_name'] for item in test_info['images']]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th item from the dataset\n",
    "        Returns\n",
    "            orig_image: original image\n",
    "            image: preprocessed image\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "        orig_image = np.array(PIL_image)\n",
    "        image = self.transform(PIL_image)\n",
    "        # return original image and pre-processed image tensor\n",
    "        return orig_image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNgfURGzB9SB"
   },
   "source": [
    "### Create Caption Mode Dataset for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qiKo09VNH4M"
   },
   "outputs": [],
   "source": [
    "# for captioning demo\n",
    "single_dataset_test = CoCoDataset_CaptionMode(transform=transform_eval,\n",
    "                                              batch_size=1,\n",
    "                                              vocab_file=train_vocab,\n",
    "                                              annotations_file='/content/drive/MyDrive/ImageCaptioning/opt/cocoapi/annotations/image_info_test2014.json',\n",
    "                                              img_folder='/content/drive/MyDrive/ImageCaptioning/opt/cocoapi/images/test2014/')\n",
    "\n",
    "dataloader_test = DataLoader(dataset=single_dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DePjETuUDvkg"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buEA_WLXWjVB"
   },
   "source": [
    "### CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KLj3v_lHWkrY"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder (CNN-based architecture)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            embed_size: (int) dimension of extracted image semantics features\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # load the pre-trained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # freeze the weights\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        # grab all CNN layers except the last one\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # embedding layers\n",
    "        self.embedding = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args\n",
    "            images: (tensor) processed image tensor. shape=(batch_size, 3, 224, 224)\n",
    "        Returns\n",
    "            feature: (tensor) extracted image semantic features. shape=(batch_size, self.embed_size)\n",
    "        \"\"\"\n",
    "        # resnet stage\n",
    "        features = self.resnet(images)\n",
    "        # flatten to 1 dim\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # embedding to final feature\n",
    "        features = self.embedding(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6b4e8664ed19460f916a9ba6c0ed4fca",
      "15c0d2fd90e741b087f9bc674417ce44",
      "e255615a2a2d47fa83fa9dc566005c73",
      "598b1d38af714f55bdae2159552a74f0",
      "0ec62d9ad3584edca32a302872e3a65c",
      "e97664995dc64e82b2201d1cc6fd98fe",
      "b3f576d75f6e48b5b5863e302bf48ff6",
      "41c7160b06f64ad9a6b87d25f48b8d09",
      "df3204b0d2324a84949df17cba1ede72",
      "de18fbc29c794906b7cf3f5d86395f3f",
      "da94ea7fbf8c4e3d99e3403cffa9813e"
     ]
    },
    "id": "c61VhpXTeApm",
    "outputId": "2fce69fd-b61c-4e9f-edd8-df8152537406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (embedding): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Create encoder\n",
    "EMBED_SIZE = 512\n",
    "\n",
    "encoder = EncoderCNN(embed_size=EMBED_SIZE)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXIwUiyLWlSa"
   },
   "source": [
    "### RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TWNOaUnhWmQB"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder (RNN-based architecture)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            embed_size: (int) dimension of extracted image semantics features\n",
    "            hidden_size: (int) dimension of decoder hidden states\n",
    "            vocab_size: (int) size of vocabulary\n",
    "            num_layers: (int) number of decoder layers\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # LSTM layer(s)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # dense layer from hidden states to vocab dimension\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args\n",
    "            features: (tensor) encoder output. shape=(batch_size, embed_size)\n",
    "            captions: (tensor) caption tokens (each element is an int). shape=(batch_size, seq_len)\n",
    "        Returns\n",
    "            fc_output: (tensor) final output. shape=(batch, vocab_size)\n",
    "        \"\"\"\n",
    "        # batch size\n",
    "        batch_size = features.shape[0]\n",
    "        # embedding dimension\n",
    "        embed_size = features.shape[1]\n",
    "        # caption length\n",
    "        seq_len = captions.shape[1]\n",
    "        # remove the <end> token\n",
    "        captions = captions[:, :-1]\n",
    "        # pass the tokenized captions into the embedding layer\n",
    "        embedded_captions = self.embedding(captions)  # (batch_size, seq_len-1, embed_size)\n",
    "        # convert features as the very first tokens\n",
    "        features = torch.unsqueeze(features, dim=1)  # (batch_size, 1, embed_size)\n",
    "        # concatenate to obtain lstm_input\n",
    "        lstm_input = torch.cat((features, embedded_captions), dim=1)  # (batch_size, seq_len, embed_size)\n",
    "        # LSTM layer\n",
    "        lstm_output, lstm_hidden = self.lstm(lstm_input)\n",
    "        # dense layer\n",
    "        fc_output = self.fc(lstm_output)\n",
    "        return fc_output\n",
    "        \n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        Decode an image from the embedded feature tensor.\n",
    "        Args\n",
    "            inputs: (tensor) embedded image features. shape=(1, 1, embed_size)\n",
    "            states: (tensor) hidden states of LSTM. shape=(1, hidden_size)\n",
    "            max_len: (int) maximum length of predicted token list\n",
    "        Returns\n",
    "            tokens: (list) a list of tokens predicted by decoder\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        x = inputs\n",
    "        # output tokens one by one\n",
    "        for _ in range(max_len):\n",
    "            # lstm layer\n",
    "            x, states = self.lstm(x, states)  # (batch_size=1, 1, hidden_size)\n",
    "            # dense layer\n",
    "            x = self.fc(x)  # (batch_size=1, 1, vocab_size)\n",
    "            # token\n",
    "            tok = torch.argmax(x, dim=-1)  # (batch_size=1, 1)\n",
    "            # append to the output\n",
    "            tokens.append(int(tok[0, 0]))\n",
    "            # early stop (token == 1)\n",
    "            if tok[0, 0] == 1:\n",
    "                break\n",
    "            # embedding\n",
    "            x = self.embedding(tok)  # (batch_size, 1, embed_size)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydFZCrGDlH0I",
    "outputId": "2b355eb8-6898-41bf-ef6f-7326e14c333b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(8852, 512)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=8852, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Create decoder\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "decoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eBvDBAqWnCj"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L9pmJpNAWn7S"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, enc, dec, num_epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    Args\n",
    "        model_name: (str) unique name of the model to save\n",
    "        enc: Pytorch DL model for encoder\n",
    "        dec: Pytorch DL model for decoder\n",
    "        num_epochs: number of epochs to train\n",
    "        criterion: the loss function object\n",
    "        optimizer: the optimizer\n",
    "    \"\"\"\n",
    "    # number of steps per epoch\n",
    "    train_steps_per_epoch = math.ceil(len(dataloader_train.dataset.caption_lengths)/dataloader_train.batch_sampler.batch_size)\n",
    "    val_steps_per_epoch = math.ceil(len(dataloader_val.dataset.caption_lengths)/dataloader_val.batch_sampler.batch_size)\n",
    "\n",
    "    # iterate epoch\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \"\"\"\n",
    "        print(\"=== Training ===\")\n",
    "\n",
    "        enc.train()\n",
    "        dec.train()\n",
    "\n",
    "        for step_i in range(train_steps_per_epoch):\n",
    "            # sample training indices from dataloader_train\n",
    "            training_indices = dataloader_train.dataset.get_data_indices()\n",
    "            # batch sampler\n",
    "            new_sampler = SubsetRandomSampler(indices=training_indices)\n",
    "            # load\n",
    "            dataloader_train.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # load inputs\n",
    "            images_t, captions_t = next(iter(dataloader_train))\n",
    "\n",
    "            images_t = images_t.to(device)\n",
    "            captions_t = captions_t.to(device)\n",
    "\n",
    "            # zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # encode\n",
    "            features_t = enc(images_t)\n",
    "\n",
    "            # decode\n",
    "            outputs_t = dec(features_t, captions_t)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(outputs_t.view(-1, VOCAB_SIZE), captions_t.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # training stats\n",
    "            stats = \"Epoch {}/{}, Step {}/{}, Train Loss: {:4f}, Perplexity: {:5.4f}\".format(epoch, num_epochs, step_i, train_steps_per_epoch, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # same line print out\n",
    "            print('\\r' + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Evaluation\n",
    "        \"\"\"\n",
    "        print(\"=== Evaluation ===\")\n",
    "\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "\n",
    "        eval_total_loss = 0.0\n",
    "\n",
    "        for step_i in range(val_steps_per_epoch):\n",
    "            # sample indices\n",
    "            val_indices = dataloader_val.dataset.get_data_indices()\n",
    "            # subset sampler\n",
    "            dataloader_val.batch_sampler.sampler = SubsetRandomSampler(indices=val_indices)\n",
    "            # load inputs\n",
    "            images_t, captions_t = next(iter(dataloader_val))\n",
    "            images_t.to(device)\n",
    "            captions_t.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # encode\n",
    "                features_t = enc(images_t)\n",
    "                # decode\n",
    "                outputs_t = dec(features_t, captions_t)\n",
    "                # loss\n",
    "                loss = criterion(outputs_t.view(-1, VOCAB_SIZE), captions_t.view(-1))\n",
    "                eval_total_loss += loss.item() * features_t.size(0)\n",
    "\n",
    "            # evaluation stats\n",
    "            stats = \"Epoch {}/{}, Step {}/{}, Evaluation Loss: {:4f}, Perplexity: {:5.4f}\".format(epoch, num_epochs, step_i, val_steps_per_epoch, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # same line print out\n",
    "            print('\\r' + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        eval_mean_loss = eval_total_loss / len(dataloader_val)\n",
    "        print(\"Epoch {}/{}, Mean Eval Loss: {:4f}, Perplexity: {:5.4f}\".format(epoch, num_epochs, step_i, val_steps_per_epoch, eval_mean_loss, np.exp(eval_mean_loss)))\n",
    "\n",
    "        # save encoder\n",
    "        torch.save(enc.state_dict(), os.path.join(\"./saved_models\", \"encoder_\" + model_name + \".pth\"))\n",
    "        torch.save(dec.state_dict(), os.path.join(\"./saved_models\", \"decoder_\" + model_name + \".pth\"))\n",
    "\n",
    "    return enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SbqUf06NSC9"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "BfBEkrKnLxBV",
    "outputId": "217419de-0732-4c36-e593-ecce273b2a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "Epoch 1/1, Step 776/25883, Train Loss: 3.612163, Perplexity: 37.04613"
     ]
    }
   ],
   "source": [
    "# move model to gpu\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# unique model name for saving the weights\n",
    "model_name = \"020322\"\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# trainable parameters\n",
    "params = list(decoder.parameters()) + list(encoder.embedding.parameters())\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# train\n",
    "encoder, decoder = train_model(model_name, encoder, decoder, num_epochs=1, criterion=criterion, optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAcVwOWs8SOB"
   },
   "source": [
    "## Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blMLN31sHiTx"
   },
   "outputs": [],
   "source": [
    "def get_word_list_and_sentence(token_list):\n",
    "    \"\"\"\n",
    "    Given a list of token (ex. [1, 1024, 222, 2]):\n",
    "        1. remove the <start> token\n",
    "        2. remove the <end> token and all its following tokens\n",
    "    And finally return a list of words (ex. [\"Hello\", \"world\"]) and the complete sentence (ex. \"Hello world\")\n",
    "    Args\n",
    "        token_list: (list) a list of token integers\n",
    "    Returns\n",
    "        word_list: (list) a list of words\n",
    "        sentence: (str) a str of the words joined by spaces \n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "\n",
    "    for tok in token_list:\n",
    "        # skip the <start> token\n",
    "        if tok == 0:\n",
    "            continue\n",
    "       \n",
    "        # break if it's an <end> token\n",
    "        if tok == 1:\n",
    "            break\n",
    "        \n",
    "        # look up the word\n",
    "        word = train_vocab.idx2word[tok]\n",
    "        word_list.append(word)\n",
    "    \n",
    "    sentence = \" \".join(word_list)\n",
    "    \n",
    "    return word_list, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFTQESSt8YIk"
   },
   "outputs": [],
   "source": [
    "def eval_BLEU(encoder, decoder, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a chosen dataset to calculate the overall BLEU score\n",
    "    Args\n",
    "        encoder: (Pytorch model) encoder\n",
    "        decoder: (Pytorch model) decoder\n",
    "        dataloader: (Pytorch dataloader) single_dataloader_train OR single_dataloader_val\n",
    "    Returns\n",
    "        avg_bleu: (float) average BLEU score\n",
    "        bleu_list: (list) list of BLEUs scores for all data\n",
    "    \"\"\"\n",
    "    # turn on eval mode and move to GPU\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    # create a list to store all BLEU scores\n",
    "    bleu_list = []\n",
    "    \n",
    "    # load (processed image,  caption) one by one (batch_size is 1)\n",
    "    for image_t, caption_t, ann_idx in dataloader:\n",
    "        with torch.no_grad():\n",
    "            # encode\n",
    "            feature_t = encoder(image_t)\n",
    "            # decode\n",
    "            token_list = decoder.sample(feature_t)\n",
    "            # convert token list to word list\n",
    "            word_list, _ = get_word_list_and_sentence(token_list.detach().cpu())\n",
    "            # convert captions to word list\n",
    "            ref_list, _ = get_word_list_and_sentence(caption_t.to_list().detach().cpu())\n",
    "            # calculate BLEU\n",
    "            bleu_list.append([word_list], [ref_list])\n",
    "    \n",
    "    # calculate average BLEU score\n",
    "    avg_bleu = 1.0*sum(bleu_list)/len(bleu_list)\n",
    "    \n",
    "    return avg_bleu, bleu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGIT_HnuJXfp"
   },
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val_avgBLEU, _ = eval_BLEU(encoder, decoder, single_dataloader_val)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImgCap_ResNet_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ec62d9ad3584edca32a302872e3a65c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da94ea7fbf8c4e3d99e3403cffa9813e",
      "placeholder": "​",
      "style": "IPY_MODEL_de18fbc29c794906b7cf3f5d86395f3f",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 147MB/s]"
     }
    },
    "15c0d2fd90e741b087f9bc674417ce44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41c7160b06f64ad9a6b87d25f48b8d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "598b1d38af714f55bdae2159552a74f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df3204b0d2324a84949df17cba1ede72",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41c7160b06f64ad9a6b87d25f48b8d09",
      "value": 102530333
     }
    },
    "6b4e8664ed19460f916a9ba6c0ed4fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e255615a2a2d47fa83fa9dc566005c73",
       "IPY_MODEL_598b1d38af714f55bdae2159552a74f0",
       "IPY_MODEL_0ec62d9ad3584edca32a302872e3a65c"
      ],
      "layout": "IPY_MODEL_15c0d2fd90e741b087f9bc674417ce44"
     }
    },
    "b3f576d75f6e48b5b5863e302bf48ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da94ea7fbf8c4e3d99e3403cffa9813e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de18fbc29c794906b7cf3f5d86395f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df3204b0d2324a84949df17cba1ede72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e255615a2a2d47fa83fa9dc566005c73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f576d75f6e48b5b5863e302bf48ff6",
      "placeholder": "​",
      "style": "IPY_MODEL_e97664995dc64e82b2201d1cc6fd98fe",
      "value": "100%"
     }
    },
    "e97664995dc64e82b2201d1cc6fd98fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
