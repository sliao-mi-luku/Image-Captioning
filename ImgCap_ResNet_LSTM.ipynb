{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13QJ2U-Nc3zQ"
   },
   "source": [
    "# Image Captioning - ResNet-LSTM model\n",
    "\n",
    "\n",
    "This notebook use a ResNet-LSTM model for image captioning\n",
    "\n",
    "\n",
    "**References**\n",
    " \n",
    "1. Udacity's Computer Vision Nanodegree\n",
    "2. COCO API: https://github.com/cocodataset/cocoapi\n",
    "3. This notebook tells how to download the COCO Dataset https://colab.research.google.com/github/rammyram/image_captioning/blob/master/Image_Captioning.ipynb\n",
    "4. Google's paper using LSTM for image captioning https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "**Notes**\n",
    "\n",
    "This notebook was run on my local machine (Windows, GTX 1050Ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTqjEOdPdmRW"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7pqsXb5bTS1",
    "outputId": "6523c2fd-ac6e-4143-afbf-ddcd120da3e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PandaEgg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from PIL import Image\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/opt/cocoapi/PythonAPI\")\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs1_DKZ-DHzk"
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "We built the vocabulary from the training data set (`captions_train2014.json`).\n",
    "\n",
    "Implementation of the vocabulary object is adapted from *Udacity Computer Vision Nanodegree*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sTXk5s_9DLcb"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Vocabulary object (Implemented by Udacity)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_threshold, vocab_file='/content/vocab.pkl',\n",
    "                 start_word=\"<start>\", end_word=\"<end>\", unk_word=\"<unk>\",\n",
    "                 annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                 vocab_from_file=False):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            vocab_threshold: minimum count of the words to be considered a unique token\n",
    "            vocab_file: vocab file\n",
    "            start_word: start-of-sentence token\n",
    "            end_word: end-of-sentence token\n",
    "            unk_word: unknown-word token\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            vocab_from_file: (boolean) whether or not to use the existing vocab file\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"\n",
    "        Load/Create the vocab file\n",
    "        \"\"\"\n",
    "        # load and use the existing vocab file\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        \n",
    "        # build a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"\n",
    "        Create dicts for converting tokens to integers (and vice-versa)\n",
    "        \"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"\n",
    "        Initialize the dictionaries for converting tokens to integers (and vice-versa)\n",
    "        \"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a token to the vocabulary\n",
    "        \"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"\n",
    "        Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold\n",
    "        \"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iePu7YVxJA_8"
   },
   "source": [
    "### Build Vocab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE5sWnnVJBOd",
    "outputId": "ffb73ac0-2668-4912-8007-42d4c64ac02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.65s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "# minimum count required to add to the vocabulary list\n",
    "VOCAB_THRESHOLD = 5\n",
    "\n",
    "# build vocab file from training data\n",
    "train_vocab = Vocabulary(vocab_threshold=VOCAB_THRESHOLD,\n",
    "                         vocab_file=\"./vocab.pkl\",\n",
    "                         start_word=\"<start>\",\n",
    "                         end_word=\"<end>\",\n",
    "                         unk_word=\"<unk>\",\n",
    "                         annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnvTC_5QNYYD"
   },
   "source": [
    "## Data Transforms\n",
    "\n",
    "We create two kinds of datatransforms.\n",
    "\n",
    "`transform_train` has data augmentation (random crop and random horizontal flip)\n",
    "\n",
    "`transform_eval` does not have data augmentation at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d6KFoO1fNb9Y"
   },
   "outputs": [],
   "source": [
    "# training data transform\n",
    "transform_train = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "# validation/test data transform\n",
    "transform_eval = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsXi7zNJCkNd"
   },
   "source": [
    "## Custom COCO Datasets\n",
    "\n",
    "There are 2 types of cumstom COCO dataset.\n",
    "\n",
    "**Dev Mode**\n",
    "\n",
    "When we loop over the `CoCoDataset_DevMode` dataset, the **processed image tensor** and the **tokenized captions** will be extracted\n",
    "\n",
    "**Caption Mode**\n",
    "\n",
    "When we loop over the `CoCoDataset_CaptionMode` dataset, the **original image matrix** (ndarray) and the **processed image tensor** will be extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-fE0SOCBFy6"
   },
   "source": [
    "### COCO Dataset (Dev Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pbPKNV6xITy4"
   },
   "outputs": [],
   "source": [
    "class CoCoDataset_DevMode(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, batch_size, vocab_file, annotations_file, img_folder):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            transform: data transform\n",
    "            batch_size: batch size\n",
    "            vocab_file: path to the existing vocab file\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            img_folder: path to the images\n",
    "        \"\"\"\n",
    "        # data transform\n",
    "        self.transform = transform\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # vocab\n",
    "        self.vocab = vocab_file\n",
    "        # image folder\n",
    "        self.img_folder = img_folder\n",
    "        # initialize COCO\n",
    "        self.coco = COCO(annotations_file)\n",
    "        # annotation ids\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        print('Obtaining caption lengths...')\n",
    "        all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "        self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th item from the dataset\n",
    "        Returns\n",
    "            image: processed image\n",
    "            caption: tokenized caption (including special tokens)\n",
    "        \"\"\"\n",
    "        ann_id = self.ids[idx]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        # Convert caption to tensor of word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab(self.vocab.start_word))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab(self.vocab.end_word))\n",
    "        caption = torch.Tensor(caption).long()\n",
    "        # return pre-processed image and caption tensors\n",
    "        return image, caption\n",
    "\n",
    "    def get_data_indices(self):\n",
    "        # choose a length of the caption\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        # find all availalbe captions with this length\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        # select batch_size captions among them\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlyeVXa2BMt6"
   },
   "source": [
    "### Create Dev Mode Dataset for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fIikCa0BQBj",
    "outputId": "f4ff8c59-5f90-47da-fa70-7a54da21b8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.69s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:43<00:00, 9415.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 202654/202654 [00:21<00:00, 9336.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# training dataset for training purposes\n",
    "dataset_train = CoCoDataset_DevMode(transform=transform_train,  # with augmentation\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      vocab_file=train_vocab,\n",
    "                                      annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                                      img_folder=\"./images/train2014/\")\n",
    "\n",
    "training_batch_sampler = BatchSampler(sampler=SubsetRandomSampler(indices=dataset_train.get_data_indices()),\n",
    "                                      batch_size=dataset_train.batch_size,\n",
    "                                      drop_last=False)\n",
    "\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_sampler=training_batch_sampler)\n",
    "\n",
    "\n",
    "# validation dataset for training purposes\n",
    "dataset_val = CoCoDataset_DevMode(transform=transform_eval,  # no augmentation\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  vocab_file=train_vocab,\n",
    "                                  annotations_file=\"./opt/cocoapi/annotations/captions_val2014.json\",\n",
    "                                  img_folder=\"./images/val2014/\")\n",
    "\n",
    "val_batch_sampler = BatchSampler(sampler=SubsetRandomSampler(indices=dataset_val.get_data_indices()),\n",
    "                                 batch_size=dataset_val.batch_size,\n",
    "                                 drop_last=False)\n",
    "\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_sampler=val_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN8C4TwVC_0w",
    "outputId": "1b062af0-ad95-416e-dd9c-a50b16a7d345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 8852\n"
     ]
    }
   ],
   "source": [
    "# size of vocab\n",
    "VOCAB_SIZE = len(dataloader_train.dataset.vocab)\n",
    "print(\"VOCAB_SIZE: {}\".format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buEA_WLXWjVB"
   },
   "source": [
    "## CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KLj3v_lHWkrY"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder (CNN-based architecture)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            embed_size: (int) dimension of extracted image semantics features\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # load the pre-trained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # freeze the weights\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        # grab all CNN layers except the last one\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # embedding layers\n",
    "        self.embedding = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args\n",
    "            images: (tensor) processed image tensor. shape=(batch_size, 3, 224, 224)\n",
    "        Returns\n",
    "            feature: (tensor) extracted image semantic features. shape=(batch_size, self.embed_size)\n",
    "        \"\"\"\n",
    "        # resnet stage\n",
    "        features = self.resnet(images)\n",
    "        # flatten to 1 dim\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # embedding to final feature\n",
    "        features = self.embedding(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXIwUiyLWlSa"
   },
   "source": [
    "## RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TWNOaUnhWmQB"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder (RNN-based architecture)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            embed_size: (int) dimension of extracted image semantics features\n",
    "            hidden_size: (int) dimension of decoder hidden states\n",
    "            vocab_size: (int) size of vocabulary\n",
    "            num_layers: (int) number of decoder layers\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # LSTM layer(s)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # dense layer from hidden states to vocab dimension\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args\n",
    "            features: (tensor) encoder output. shape=(batch_size, embed_size)\n",
    "            captions: (tensor) caption tokens (each element is an int). shape=(batch_size, seq_len)\n",
    "        Returns\n",
    "            fc_output: (tensor) final output. shape=(batch, vocab_size)\n",
    "        \"\"\"\n",
    "        # batch size\n",
    "        batch_size = features.shape[0]\n",
    "        # embedding dimension\n",
    "        embed_size = features.shape[1]\n",
    "        # caption length\n",
    "        seq_len = captions.shape[1]\n",
    "        # remove the <end> token\n",
    "        captions = captions[:, :-1]\n",
    "        # pass the tokenized captions into the embedding layer\n",
    "        embedded_captions = self.embedding(captions)  # (batch_size, seq_len-1, embed_size)\n",
    "        # convert features as the very first tokens\n",
    "        features = torch.unsqueeze(features, dim=1)  # (batch_size, 1, embed_size)\n",
    "        # concatenate to obtain lstm_input\n",
    "        lstm_input = torch.cat((features, embedded_captions), dim=1)  # (batch_size, seq_len, embed_size)\n",
    "        # LSTM layer\n",
    "        lstm_output, lstm_hidden = self.lstm(lstm_input)\n",
    "        # dense layer\n",
    "        fc_output = self.fc(lstm_output)\n",
    "        return fc_output\n",
    "        \n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        Decode an image from the embedded feature tensor.\n",
    "        Args\n",
    "            inputs: (tensor) embedded image features. shape=(1, 1, embed_size)\n",
    "            states: (tensor) hidden states of LSTM. shape=(1, hidden_size)\n",
    "            max_len: (int) maximum length of predicted token list\n",
    "        Returns\n",
    "            tokens: (list) a list of tokens predicted by decoder\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        x = inputs\n",
    "        # output tokens one by one\n",
    "        for _ in range(max_len):\n",
    "            # lstm layer\n",
    "            x, states = self.lstm(x, states)  # (batch_size=1, 1, hidden_size)\n",
    "            # dense layer\n",
    "            x = self.fc(x)  # (batch_size=1, 1, vocab_size)\n",
    "            # token\n",
    "            tok = torch.argmax(x, dim=-1)  # (batch_size=1, 1)\n",
    "            # append to the output\n",
    "            tokens.append(int(tok[0, 0]))\n",
    "            # early stop (token == 1)\n",
    "            if tok[0, 0] == 1:\n",
    "                break\n",
    "            # embedding\n",
    "            x = self.embedding(tok)  # (batch_size, 1, embed_size)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eBvDBAqWnCj"
   },
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L9pmJpNAWn7S"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, enc, dec, num_epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    Args\n",
    "        model_name: (str) unique name of the model to save\n",
    "        enc: Pytorch DL model for encoder\n",
    "        dec: Pytorch DL model for decoder\n",
    "        num_epochs: number of epochs to train\n",
    "        criterion: the loss function object\n",
    "        optimizer: the optimizer\n",
    "    \"\"\"\n",
    "    # number of steps per epoch\n",
    "    train_steps_per_epoch = math.ceil(len(dataloader_train.dataset.caption_lengths)/dataloader_train.batch_sampler.batch_size)\n",
    "    val_steps_per_epoch = math.ceil(len(dataloader_val.dataset.caption_lengths)/dataloader_val.batch_sampler.batch_size)\n",
    "    \n",
    "    \n",
    "    # iterate epoch\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \"\"\"\n",
    "        print(\"=== Training ===\")\n",
    "\n",
    "        enc.train()\n",
    "        dec.train()\n",
    "        \n",
    "        train_total_loss = 0.0\n",
    "        train_n = 0\n",
    "\n",
    "        for step_i in range(1, train_steps_per_epoch+1):\n",
    "            # sample training indices from dataloader_train\n",
    "            training_indices = dataloader_train.dataset.get_data_indices()\n",
    "            # batch sampler\n",
    "            new_sampler = SubsetRandomSampler(indices=training_indices)\n",
    "            # load\n",
    "            dataloader_train.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # load inputs\n",
    "            images_t, captions_t = next(iter(dataloader_train))\n",
    "\n",
    "            images_t = images_t.to(device)\n",
    "            captions_t = captions_t.to(device)\n",
    "\n",
    "            # zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # encode\n",
    "            features_t = enc(images_t)\n",
    "\n",
    "            # decode\n",
    "            outputs_t = dec(features_t, captions_t)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(outputs_t.view(-1, VOCAB_SIZE), captions_t.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_total_loss += loss.item() * features_t.size(0)\n",
    "            # number of data seen\n",
    "            train_n += features_t.size(0)\n",
    "            # mean loss\n",
    "            train_mean_loss = train_total_loss / train_n\n",
    "\n",
    "            # training stats\n",
    "            stats = \"Epoch {}/{}  Step {}/{}\\tLoss: {:.4f}  Perplexity: {:.4f}\\t[Overall] Loss: {:.4f}\\tPerplexity: {:.4f}\".format(\n",
    "                epoch, num_epochs, step_i, train_steps_per_epoch, loss.item(), np.exp(loss.item()), train_mean_loss, np.exp(train_mean_loss))\n",
    "\n",
    "            # same line print out\n",
    "            print('\\r' + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if step_i == train_steps_per_epoch:\n",
    "                print('\\r' + stats)\n",
    "            \n",
    "        \"\"\"\n",
    "        Evaluation\n",
    "        \"\"\"\n",
    "        print(\"=== Evaluation ===\")\n",
    "\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "\n",
    "        eval_total_loss = 0.0\n",
    "        eval_n = 0\n",
    "\n",
    "        for step_i in range(1, val_steps_per_epoch+1):\n",
    "            # sample indices\n",
    "            val_indices = dataloader_val.dataset.get_data_indices()\n",
    "            # subset sampler\n",
    "            dataloader_val.batch_sampler.sampler = SubsetRandomSampler(indices=val_indices)\n",
    "            # load inputs\n",
    "            images_t, captions_t = next(iter(dataloader_val))\n",
    "            images_t = images_t.to(device)\n",
    "            captions_t = captions_t.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # encode\n",
    "                features_t = enc(images_t)\n",
    "                # decode\n",
    "                outputs_t = dec(features_t, captions_t)\n",
    "                # loss\n",
    "                loss = criterion(outputs_t.view(-1, VOCAB_SIZE), captions_t.view(-1))\n",
    "                eval_total_loss += loss.item() * features_t.size(0)\n",
    "                # number of data seen\n",
    "                eval_n += features_t.size(0)\n",
    "                # mean loss\n",
    "                eval_mean_loss = eval_total_loss / eval_n\n",
    "\n",
    "            # evaluation stats\n",
    "            stats = \"Epoch {}/{}  Step {}/{}\\tLoss: {:.4f}  Perplexity: {:.4f}\\t[Overall] Loss: {:.4f}\\tPerplexity: {:.4f}\".format(\n",
    "                epoch, num_epochs, step_i, val_steps_per_epoch, loss.item(), np.exp(loss.item()), eval_mean_loss, np.exp(eval_mean_loss))\n",
    "\n",
    "            # same line print out\n",
    "            print('\\r' + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            if step_i == val_steps_per_epoch:\n",
    "                print('\\r' + stats)\n",
    "            \n",
    "        # save encoder\n",
    "        torch.save(enc.state_dict(), os.path.join(\"./saved_models\", \"encoder_\" + model_name + \".pth\"))\n",
    "        torch.save(dec.state_dict(), os.path.join(\"./saved_models\", \"decoder_\" + model_name + \".pth\"))\n",
    "\n",
    "    return enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SbqUf06NSC9"
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (embedding): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Create encoder\n",
    "EMBED_SIZE = 512\n",
    "encoder = EncoderCNN(embed_size=EMBED_SIZE)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(8852, 512)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=8852, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Create decoder\n",
    "HIDDEN_SIZE = 512\n",
    "decoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "BfBEkrKnLxBV",
    "outputId": "217419de-0732-4c36-e593-ecce273b2a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "Epoch 1/1  Step 12942/12942\tLoss: 2.1735  Perplexity: 8.7892\t[Overall] Loss: 2.4839\tPerplexity: 11.988186\n",
      "=== Evaluation ===\n",
      "Epoch 1/1  Step 6333/6333\tLoss: 1.9244  Perplexity: 6.8512\t[Overall] Loss: 2.3548\tPerplexity: 10.53642\n"
     ]
    }
   ],
   "source": [
    "# move to gpu\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# unique model name for saving the weights\n",
    "model_name = \"020322\"\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# trainable parameters\n",
    "params = list(decoder.parameters()) + list(encoder.embedding.parameters())\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# train\n",
    "encoder, decoder = train_model(model_name, encoder, decoder, num_epochs=1, criterion=criterion, optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAcVwOWs8SOB"
   },
   "source": [
    "## Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoDataset_BLEUMode(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, batch_size, vocab_file, annotations_file, img_folder):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            transform: data transform\n",
    "            batch_size: batch size\n",
    "            vocab_file: path to the existing vocab file\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            img_folder: path to the images\n",
    "        \"\"\"\n",
    "        # data transform\n",
    "        self.transform = transform\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # vocab\n",
    "        self.vocab = vocab_file\n",
    "        # image folder\n",
    "        self.img_folder = img_folder\n",
    "        # initialize COCO\n",
    "        self.coco = COCO(annotations_file)\n",
    "        # annotation ids\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        print('Obtaining caption lengths...')\n",
    "        all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "        self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th item from the dataset\n",
    "        Returns\n",
    "            image: processed image\n",
    "            caption: tokenized caption (including special tokens)\n",
    "        \"\"\"\n",
    "        ann_id = self.ids[idx]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        # Convert caption to tensor of word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab(self.vocab.start_word))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab(self.vocab.end_word))\n",
    "        caption = torch.Tensor(caption).long()\n",
    "        # return pre-processed image and caption tensors\n",
    "        return image, caption, img_id\n",
    "\n",
    "    def get_data_indices(self):\n",
    "        # choose a length of the caption\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        # find all availalbe captions with this length\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        # select batch_size captions among them\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.50s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:52<00:00, 7870.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 202654/202654 [00:24<00:00, 8162.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset for BLEU calculation\n",
    "single_dataset_train = CoCoDataset_BLEUMode(transform=transform_eval,  # no augmentation\n",
    "                                           batch_size=1,\n",
    "                                           vocab_file=train_vocab,\n",
    "                                           annotations_file=\"./opt/cocoapi/annotations/captions_train2014.json\",\n",
    "                                           img_folder=\"./images/train2014/\")\n",
    "\n",
    "single_dataloader_train = DataLoader(dataset=single_dataset_train, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# for BLEU calculation\n",
    "single_dataset_val = CoCoDataset_BLEUMode(transform=transform_eval,  # no augmentation\n",
    "                                         batch_size=1,\n",
    "                                         vocab_file=train_vocab,\n",
    "                                         annotations_file=\"./opt/cocoapi/annotations/captions_val2014.json\",\n",
    "                                         img_folder=\"./images/val2014/\")\n",
    "\n",
    "single_dataloader_val = DataLoader(dataset=single_dataset_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "blMLN31sHiTx"
   },
   "outputs": [],
   "source": [
    "def get_word_list_and_sentence(token_list):\n",
    "    \"\"\"\n",
    "    Given a list of token (ex. [1, 1024, 222, 2]):\n",
    "        1. remove the <start> token\n",
    "        2. remove the <end> token and all its following tokens\n",
    "    And finally return a list of words (ex. [\"Hello\", \"world\"]) and the complete sentence (ex. \"Hello world\")\n",
    "    Args\n",
    "        token_list: (list) a list of token integers\n",
    "    Returns\n",
    "        word_list: (list) a list of words\n",
    "        sentence: (str) a str of the words joined by spaces \n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "\n",
    "    for tok in token_list:\n",
    "        # skip the <start> token\n",
    "        if tok == 0:\n",
    "            continue\n",
    "       \n",
    "        # break if it's an <end> token\n",
    "        if tok == 1:\n",
    "            break\n",
    "        \n",
    "        # look up the word\n",
    "        word = train_vocab.idx2word[tok]\n",
    "        word_list.append(word)\n",
    "    \n",
    "    sentence = \" \".join(word_list)\n",
    "    \n",
    "    return word_list, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_test(dataloader):\n",
    "    img_ids = []\n",
    "    for _, _, img_id in dataloader:\n",
    "        img_ids.append(img_id.tolist()[0])\n",
    "    return img_ids\n",
    "\n",
    "x = unique_test(single_dataloader_val)\n",
    "print(len(x))\n",
    "print(len(set(x)))\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "rFTQESSt8YIk"
   },
   "outputs": [],
   "source": [
    "def eval_BLEU(encoder, decoder, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a chosen dataset to calculate the overall BLEU score\n",
    "    Args\n",
    "        encoder: (Pytorch model) encoder\n",
    "        decoder: (Pytorch model) decoder\n",
    "        dataloader: (Pytorch dataloader) single_dataloader_train OR single_dataloader_val\n",
    "    Returns\n",
    "        avg_bleu: (float) average BLEU score\n",
    "        bleu_list: (list) list of BLEUs scores for all data\n",
    "    \"\"\"\n",
    "    # turn on eval mode and move to GPU\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    # \n",
    "    dict_candidates = dict()\n",
    "    dict_references = dict()\n",
    "    \n",
    "    \n",
    "    # create a list to store all BLEU scores\n",
    "    bleu_list = []\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    # load (processed image,  caption) one by one (batch_size is 1)\n",
    "    for image_t, caption_t, img_id in dataloader:\n",
    "        \n",
    "        image_t = image_t.to(device)\n",
    "        caption_t = caption_t.to(device)\n",
    "        \n",
    "        img_id = img_id.tolist()[0]  # int\n",
    "        \n",
    "        cnt += caption_t.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # encode\n",
    "            feature_t = encoder(image_t).unsqueeze(1)\n",
    "            # decode\n",
    "            token_list = decoder.sample(feature_t)\n",
    "            \n",
    "            # convert token list to word list\n",
    "            decoded_word_list, decoded_sentence = get_word_list_and_sentence(token_list)\n",
    "            if decoded_sentence not in dict_candidates.get(img_id, []):\n",
    "                dict_candidates[img_id] = dict_candidates.get(img_id, []) + [decoded_word_list]\n",
    "            \n",
    "            # convert captions to word list\n",
    "            ref_word_list, ref_sentence = get_word_list_and_sentence(caption_t.tolist()[0])\n",
    "            if ref_sentence not in dict_references.get(img_id, []):\n",
    "                dict_references[img_id] = dict_references.get(img_id, []) + [ref_word_list]\n",
    "                \n",
    "        stats = \"[{}/{}] Calculating BLEU scores...\".format(cnt, len(dataloader.dataset))\n",
    "        # same line print out\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if cnt == len(dataloader.dataset):\n",
    "            print('\\r' + stats)\n",
    "            break\n",
    "    \n",
    "    # calculate BLEU\n",
    "    bleu_candidates = []\n",
    "    bleu_references = []\n",
    "    for img_id in dict_candidates.keys():\n",
    "        \n",
    "        for cancadate in dict_candidates[img_id]:\n",
    "            bleu_candidates.append(cancadate)\n",
    "            bleu_references.append(dict_references[img_id])\n",
    "    \n",
    "    bleu1 = bleu_score(bleu_candidates, bleu_references, max_n=4, weights=[1.0, 0.0, 0.0, 0.0])\n",
    "    bleu2 = bleu_score(bleu_candidates, bleu_references, max_n=4, weights=[0.5, 0.5, 0.0, 0.0])\n",
    "    bleu3 = bleu_score(bleu_candidates, bleu_references, max_n=4, weights=[0.33, 0.33, 0.33, 0.0])\n",
    "    bleu4 = bleu_score(bleu_candidates, bleu_references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    bleu = [bleu1, bleu2, bleu3, bleu4]\n",
    "    \n",
    "    return bleu, bleu_candidates, bleu_references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414113/414113] Calculating BLEU scores...\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "bleu, _, _ = eval_BLEU(encoder, decoder, single_dataloader_train)\n",
    "print(\"\\n\")\n",
    "print(\"BLEU Scores: {}\".format(bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "eGIT_HnuJXfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202654/202654] Calculating BLEU scores...\n",
      "\n",
      "\n",
      "Mean validation BLEU Score: [0.6055186931482363, 0.3844462650303808, 0.24774613448862315, 0.15984731769752322]\n",
      "BLEU Scores: [0.6055186931482363, 0.3844462650303808, 0.24774613448862315, 0.15984731769752322]\n"
     ]
    }
   ],
   "source": [
    "# validation dataset\n",
    "bleu, _, _ = eval_BLEU(encoder, decoder, single_dataloader_val)\n",
    "print(\"\\n\")\n",
    "print(\"BLEU Scores: {}\".format(bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQxJfjtCJ_gu"
   },
   "outputs": [],
   "source": [
    "class CoCoDataset_CaptionMode(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, batch_size, vocab_file, annotations_file, img_folder):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            transform: data transform\n",
    "            batch_size: batch size\n",
    "            vocab_file: path to the existing vocab file\n",
    "            annotations_file: annotations file (for training dataset)\n",
    "            img_folder: path to the images\n",
    "        \"\"\"\n",
    "        # data transform\n",
    "        self.transform = transform\n",
    "         # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # vocab\n",
    "        self.vocab = vocab_file\n",
    "        # image folder\n",
    "        self.img_folder = img_folder\n",
    "        # path\n",
    "        test_info = json.loads(open(annotations_file).read())\n",
    "        self.paths = [item['file_name'] for item in test_info['images']]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th item from the dataset\n",
    "        Returns\n",
    "            orig_image: original image\n",
    "            image: preprocessed image\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "        orig_image = np.array(PIL_image)\n",
    "        image = self.transform(PIL_image)\n",
    "        # return original image and pre-processed image tensor\n",
    "        return orig_image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qiKo09VNH4M"
   },
   "outputs": [],
   "source": [
    "# for captioning demo\n",
    "single_dataset_test = CoCoDataset_CaptionMode(transform=transform_eval,\n",
    "                                              batch_size=1,\n",
    "                                              vocab_file=train_vocab,\n",
    "                                              annotations_file=\"./opt/cocoapi/annotations/image_info_test2014.json\",\n",
    "                                              img_folder=\"./images/test2014/\")\n",
    "\n",
    "dataloader_test = DataLoader(dataset=single_dataset_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_testdata(dataloader, encoder, decoder):\n",
    "    \"\"\"\n",
    "    Random sample an image from the test data and generate the caption along with it\n",
    "    Args\n",
    "        dataloader: (Pytorch dataloader) the test dataloader (batch_size = 1)\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    # sample an image\n",
    "    orig_image, image_t = next(iter(dataloader))\n",
    "    image_t = image_t.to(device)\n",
    "    \n",
    "    # plot the original image\n",
    "    plt.imshow(orig_image[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # caption prediction\n",
    "    with torch.no_grad():\n",
    "        features_t = encoder(image_t).unsqueeze(1)\n",
    "        token_list = decoder.sample(features_t)\n",
    "        \n",
    "    decoded_word_list, decoded_sentence = get_word_list_and_sentence(token_list)\n",
    "    \n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image\n",
    "random_sample_testdata(single_dataset_test, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image\n",
    "random_sample_testdata(single_dataset_test, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image\n",
    "random_sample_testdata(single_dataset_test, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions for Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_captioning_custom_image(img_path, encoder, decoder):\n",
    "    \"\"\"\n",
    "    Generate a caption for a custom image\n",
    "    Args\n",
    "        img_path: (str) path to the image\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    # image preprocessing\n",
    "    orig_image = np.array(Image.open(img_path).convert('RGB'))\n",
    "    # plot the original image\n",
    "    plt.imshow(orig_image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # caption prediction\n",
    "    image_t = transform_eval(Image.open(img_path).convert('RGB'))\n",
    "    image_t = image_t.to(device)\n",
    "    with torch.no_grad():\n",
    "        features_t = encoder(image_t).unsqueeze(1)\n",
    "        token_list = decoder.sample(features_t)\n",
    "    decoded_word_list, decoded_sentence = get_word_list_and_sentence(token_list)\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example custom image\n",
    "image_captioning_custom_image(\"./custom_test_images/IMG_2952.jpg\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example custom image\n",
    "image_captioning_custom_image(\"./custom_test_images/IMG_4191.jpg\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example custom image\n",
    "image_captioning_custom_image(\"./custom_test_images/IMG_4380.jpg\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example custom image\n",
    "image_captioning_custom_image(\"./custom_test_images/IMG_4446.jpg\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImgCap_ResNet_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ec62d9ad3584edca32a302872e3a65c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da94ea7fbf8c4e3d99e3403cffa9813e",
      "placeholder": "​",
      "style": "IPY_MODEL_de18fbc29c794906b7cf3f5d86395f3f",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 147MB/s]"
     }
    },
    "15c0d2fd90e741b087f9bc674417ce44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41c7160b06f64ad9a6b87d25f48b8d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "598b1d38af714f55bdae2159552a74f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df3204b0d2324a84949df17cba1ede72",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41c7160b06f64ad9a6b87d25f48b8d09",
      "value": 102530333
     }
    },
    "6b4e8664ed19460f916a9ba6c0ed4fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e255615a2a2d47fa83fa9dc566005c73",
       "IPY_MODEL_598b1d38af714f55bdae2159552a74f0",
       "IPY_MODEL_0ec62d9ad3584edca32a302872e3a65c"
      ],
      "layout": "IPY_MODEL_15c0d2fd90e741b087f9bc674417ce44"
     }
    },
    "b3f576d75f6e48b5b5863e302bf48ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da94ea7fbf8c4e3d99e3403cffa9813e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de18fbc29c794906b7cf3f5d86395f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df3204b0d2324a84949df17cba1ede72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e255615a2a2d47fa83fa9dc566005c73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f576d75f6e48b5b5863e302bf48ff6",
      "placeholder": "​",
      "style": "IPY_MODEL_e97664995dc64e82b2201d1cc6fd98fe",
      "value": "100%"
     }
    },
    "e97664995dc64e82b2201d1cc6fd98fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
